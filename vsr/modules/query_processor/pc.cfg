[InputFiles]

# This is an XML file containing the queries to be run.
LEIA = ../../data/cfquery.xml

# A CSV representation of the inverted index is needed to find out the indexes for query tokens
INVERTED_INDEX = ../inverted_index/output/output.csv

[OutputFiles]

# This is a CSV file where the queries read form the previous file will be rewritten,
# using the same filters used for processing documents, in order to normalize them.
CONSULTAS = output/vector_queries.csv

# The expected results also need to be converted to our representation. They can be obtained 
# from the query XML file. They will be placed in the following file:
RESULTADOS = output/expected_query_results.csv

# I'll store each query id and the full query text here for future purposes
RAW_QUERIES = output/raw_queries.csv

# I'll store each query id and the query tokens for future purposes
TOKENIZED_QUERIES = output/tokenized_queries.csv

[Params]

# Whether or not to use Porter's stemmer when processing queries
USE_STEMMER =True

# Tokens shorter than this value will not be considered when processing queries
TOKEN_LENGTH_THRESHOLD = 2

# If True, only Tokens where all characters are letters (no digits or special characters) will be
#   considered when processing queries.
ONLY_LETTERS = True

# If True, words in nltk's stop word list (augmented with a few custom words) will be stripped from the processed queries.
IGNORE_STOP_WORDS = True
